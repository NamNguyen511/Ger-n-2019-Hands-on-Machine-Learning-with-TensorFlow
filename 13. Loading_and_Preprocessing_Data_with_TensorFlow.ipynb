{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data API\n",
    "\n",
    "The whole Data API revolves around the concept of a *dataset* which represents a sequence of data items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset\n",
    "\n",
    "\"\"\"\n",
    "Usually you will use datasets that gradually read from disk, but the dataset you saw above is created entirely in RAM.\n",
    "The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset whose elements are all the slices of X\n",
    "\"\"\"\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7, drop_remainder=True)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "    1. repeat(n) method: it returns a new dataset that will repeat the items of the original dataset n times. You can call this method with no arguments, the new dataset will repeat the source dataset forever, so the code that iterates over the dataset will have to decide when to stop\n",
    "\n",
    "    2. batch(n) method: it will group the items of the previous dataset in batches of n times, drop_reminder=True will be called if you want to drop the batch don't have the exact same size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Creating new dataset with map() method\n",
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-a1725821a0f0>:6: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.unbatch()`.\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Creating new dataset with apply() method\n",
    "\"\"\" \n",
    "The map() method applies a transformation to each item, the apply() method applies a transformation to the dataset as a whole\n",
    "\"\"\"\n",
    "\n",
    "dataset = dataset.apply(tf.data.experimental.unbatch())\n",
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle() method: it will create a new dataset that will start by filling up a buffer with the first items of the source dataset. Then, whenever it is asked for an item, it will pull one out randomly from the buffer and replace it with a fresh one from the source datatset. \n",
    "\n",
    ".The buffer_size must be specified, and it is important to make it large enough, or else shuffling will not be very effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7], shape=(5,), dtype=int64)\n",
      "tf.Tensor([9 1 8 4 5], shape=(5,), dtype=int64)\n",
      "tf.Tensor([3 5 2 1 8], shape=(5,), dtype=int64)\n",
      "tf.Tensor([4 0 7 9 6], shape=(5,), dtype=int64)\n",
      "tf.Tensor([2 1 3 5 8], shape=(5,), dtype=int64)\n",
      "tf.Tensor([9 4 6 0 7], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).repeat(3).batch(5)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interleaving lines from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load California housing dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
      "   0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
      "   0.8252202 ]\n",
      " [-0.40342651  2.99178419 -1.33391162 -0.25683275 -1.21518188  1.89434613\n",
      "  -1.91036058  1.24758524 -0.85646254 -0.34843254 -1.71818909  0.43190599\n",
      "  -1.32920239]\n",
      " [ 0.1249402  -0.48361547  1.0283258  -0.25683275  0.62864202 -1.82968811\n",
      "   1.11048828 -1.18743907  1.67588577  1.5652875   0.78447637  0.22061726\n",
      "  -1.30850006]\n",
      " [-0.40149354 -0.48361547 -0.86940196 -0.25683275 -0.3615597  -0.3245576\n",
      "  -1.23667187  1.10717989 -0.51114231 -1.094663    0.78447637  0.44807713\n",
      "  -0.65292624]\n",
      " [-0.0056343  -0.48361547  1.0283258  -0.25683275  1.32861221  0.15364225\n",
      "   0.69480801 -0.57857203  1.67588577  1.5652875   0.78447637  0.3898823\n",
      "   0.26349695]\n",
      " [-0.37502238 -0.48361547 -0.54747912 -0.25683275 -0.54935658 -0.78865126\n",
      "   0.18954148  0.48371503 -0.51114231 -0.71552978  0.51145832  0.38669063\n",
      "  -0.13812828]\n",
      " [ 0.58963463 -0.48361547  1.0283258  -0.25683275  1.21764133 -1.03127774\n",
      "   1.11048828 -1.06518235  1.67588577  1.5652875   0.78447637  0.44807713\n",
      "   1.49873604]\n",
      " [ 0.0381708  -0.48361547  1.24588095 -0.25683275  2.67733525 -1.12719983\n",
      "   1.11048828 -1.14833073 -0.51114231 -0.01744323 -1.71818909  0.44807713\n",
      "   1.88793986]\n",
      " [-0.17228416 -0.48361547  1.24588095 -0.25683275  2.67733525 -0.90150078\n",
      "   1.11048828 -1.09664657 -0.51114231 -0.01744323 -1.71818909 -1.97365769\n",
      "   0.53952803]\n",
      " [-0.22932104 -0.48361547  1.58544339 -0.25683275  0.56888847 -1.76056777\n",
      "   1.11048828 -1.13471925 -0.62624905  0.18716835  1.23950646  0.44807713\n",
      "   2.99068404]]\n"
     ]
    }
   ],
   "source": [
    "# Scale the dataset to normalize the input values\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_full)\n",
    "X_train_new = scaler.transform(X_train_full)\n",
    "X_test_new = scaler.transform(X_test)\n",
    "print(X_train_new[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_new, y_train_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done writing files. Traning file paths: ['datasets\\\\boston\\\\my_train_00.csv', 'datasets\\\\boston\\\\my_train_01.csv', 'datasets\\\\boston\\\\my_train_02.csv', 'datasets\\\\boston\\\\my_train_03.csv', 'datasets\\\\boston\\\\my_train_04.csv', 'datasets\\\\boston\\\\my_train_05.csv', 'datasets\\\\boston\\\\my_train_06.csv', 'datasets\\\\boston\\\\my_train_07.csv', 'datasets\\\\boston\\\\my_train_08.csv', 'datasets\\\\boston\\\\my_train_09.csv', 'datasets\\\\boston\\\\my_train_10.csv', 'datasets\\\\boston\\\\my_train_11.csv', 'datasets\\\\boston\\\\my_train_12.csv', 'datasets\\\\boston\\\\my_train_13.csv', 'datasets\\\\boston\\\\my_train_14.csv', 'datasets\\\\boston\\\\my_train_15.csv', 'datasets\\\\boston\\\\my_train_16.csv', 'datasets\\\\boston\\\\my_train_17.csv', 'datasets\\\\boston\\\\my_train_18.csv', 'datasets\\\\boston\\\\my_train_19.csv']\n"
     ]
    }
   ],
   "source": [
    "#Save dataset to multiple files\n",
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join('datasets', \"boston\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding='utf-8') as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths\n",
    "\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, 'train', None, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, 'valid', None, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, 'test', None, n_parts=10)\n",
    "print('\\nDone writing files. Traning file paths:', train_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataset containing only these filepaths above:\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
    "\n",
    "\"\"\"\n",
    "By default, the list_files() function returns a dataset that shuffles the filepaths\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-0.3919630095351413,0.7801662253156655,-0.9076211063403838,-0.2568327484687563,-1.1042109960227324,0.17762277400333037,-2.193453178811165,1.625106473803,-0.3960355701527182,-0.6372959438833558,-0.8536319333458502,0.21199974653499268,-1.0421300691886186,23.7'\n",
      "b'0.34856843562344747,-0.4836154708652843,1.0283257954396188,-0.2568327484687563,1.2176413250911147,-0.7815981666935935,1.0029847632968047,-0.8944963977369444,1.6758857724016463,1.5652874992218142,0.7844763709927688,0.42179902963935195,0.6030151782242079,15.1'\n",
      "b'0.11561228941989592,-0.4836154708652843,1.0283257954396188,-0.2568327484687563,1.3286122080855265,0.6120934353778194,0.7521432207546996,-0.5635796768907313,1.6758857724016463,1.5652874992218142,0.7844763709927688,-1.0591370863914173,0.5077844550098752,16.4'\n",
      "b'0.4670650820425747,-0.4836154708652843,1.0283257954396188,-0.2568327484687563,0.2274395999102092,-1.185035209398477,0.9456495535728953,-0.646284198628302,1.6758857724016463,1.5652874992218142,0.7844763709927688,-0.023439306969036875,0.745171185341256,13.8'\n",
      "b'-0.4012313273862551,0.6959141122369354,0.5785158091490484,-0.2568327484687563,-0.7969070123458994,0.24674310649472692,-0.5522328004642475,-0.036677404830978554,-0.6262490526587586,-0.8178355703673861,-0.12558379808424228,0.42796959678948,-0.6322239127443154,22.9'\n"
     ]
    }
   ],
   "source": [
    "n_files_in_1_read = 5\n",
    "dataset = filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_files_in_1_read)\n",
    "\n",
    "\"\"\"\n",
    "The interleave() function takes a function that returns a dataset, and applies it to each filepath in the dataset.\n",
    "The function is applied to each filepath in the dataset, and the resulting datasets are interleaved in the order of the filepaths.\n",
    "By default, interleave() does not use parallelism; it just reads one line at a time from each file, squentially.\n",
    "\"\"\"\n",
    "\n",
    "for line in dataset.take(5):\n",
    "    print(line.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Data\n",
    "n_inputs = X_train.shape[-1]\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return x , y\n",
    "\n",
    "\"\"\" \n",
    "tf.io.decode_csv() function which takes two arguments:\n",
    "    1. The line to parse\n",
    "    2. An array containing the default value for each column in the CSV file. This array tells TensorFlow not inly the default value for each column, but also the number of columns and their types.\n",
    "\n",
    "The decode__csv() function returns a list of scalar tensors, one for each column in the CSV file., but we need to return 1D tensor arrays.\n",
    "    --> Call tf.stack() on all tensor except the last one (y): this will stack these tensors into a 1D array\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5, n_read_threads=None, shuffle_buffer_size=10000, n_parse_threads=5, batch_size=5):\n",
    "    #create a dataset of filepaths\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    #Create a datasets with shuffled filepaths\n",
    "    dataset  = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    #Preprocess the data\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    #shuffle the data\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    #batch the data\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefetching\n",
    "\n",
    "While our training algorithm is working on one batch, the dataset will already be working in parallel on getting the next bacth ready --> can improve performance dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset is small enough to fit in memory --> using the dataset's cache() method to cache its content to RAM, do this:\n",
    "\n",
    "**After** loading and preprocessing the data\n",
    "\n",
    "**Before** shuffling, repeating, batching and prefetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TFRecord Format\n",
    "\n",
    "    1. It is TensorFlow's preferred format for storing large amounts of data and reading it efficiently.\n",
    "\n",
    "    2. A very simple binar format that just contains a sequence of binary records of varying sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'This is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Create a TFRecord file using the tf.io.TFRecordWriter() class\n",
    "\n",
    "with tf.io.TFRecordWriter('my_data.tfrecord') as f:\n",
    "    f.write(b'This is the first record')\n",
    "    f.write(b'This is the second record')\n",
    "\n",
    "filepaths = ['my_data.tfrecord']\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressed TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a compressed TFRecord file\n",
    "options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "with tf.io.TFRecordWriter('my_compressed.tfrecord', options=options) as f:\n",
    "    f.write(b'This is the first record')\n",
    "    f.write(b'This is the second record')\n",
    "\n",
    "# Read a compressed TFRecord file:\n",
    "dataset = tf.data.TFRecordDataset(['my_compressed.tfrecord'], compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief introduction to Protocol Buffers\n",
    "\n",
    "TFRecord files usually contain serialized protocol buffers (bộ đệm giao thức được tuần tự hóa) (also called *protobufs*) --> This is a protable, extensible, and efficient binary format developed at Google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Protobufs\n",
    "\n",
    "The main protobuf typically used in a TFRecord file is the Example protobuf, whihch represents one instance in a dataset --> contains a list of named features, where each featurecan either be a list of byte strings, a list of floats, or a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\nI\\n\\x0c\\n\\x03age\\x12\\x05\\x1a\\x03\\n\\x01\\x19\\n\\x12\\n\\x06weight\\x12\\x08\\x12\\x06\\n\\x04\\xcd\\xcc\\x82B\\n\\x11\\n\\x04name\\x12\\t\\n\\x07\\n\\x05Alice\\n\\x12\\n\\x06height\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\xe0?', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "# Create example protobuf\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            'name': Feature(bytes_list=BytesList(value=[b'Alice'])),\n",
    "            'age': Feature(int64_list=Int64List(value=[25])),\n",
    "            'height': Feature(float_list=FloatList(value=[1.75])),\n",
    "            'weight': Feature(float_list=FloatList(value=[65.4]))\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Serialize the example protobuf using SerializeToString() method\n",
    "with tf.io.TFRecordWriter('my_contact.tfrecord') as f:\n",
    "    f.write(person_example.SerializeToString())\n",
    "\n",
    "filepaths = ['my_contact.tfrecord']\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Parsing serialized protobufs\\\n",
    "\n",
    "To load the serialized Example protobufs above, we will use a tf.data.TFRecordDataset and parse each Example using tf.io.parse_single_example()\n",
    "\n",
    "tf.io.parse_string_example() requires at least to arguments:\n",
    "\n",
    "    1. A string scalar tensor containing the serialized data\n",
    "    \n",
    "    2. A description of eaxh feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Alice'\n"
     ]
    }
   ],
   "source": [
    "feature_description = {\n",
    "    'name': tf.io.FixedLenFeature([], tf.string),\n",
    "    'age': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'height': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'weight': tf.io.FixedLenFeature([], tf.float32)\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "A tf.io.FixedLenFeature() function takes two arguments:\n",
    "    1. The length of the feature (the feature's shape)\n",
    "    2. The type of the feature\n",
    "\"\"\"\n",
    "\n",
    "for serialized_example in tf.data.TFRecordDataset(filepaths):\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "\n",
    "print(parsed_example['name'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling lists of lists using the SequenceExample Protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureList = tf.train.FeatureList\n",
    "FeatureLists = tf.train.FeatureLists\n",
    "SequenceExample = tf.train.SequenceExample\n",
    "\n",
    "context = Features(feature={\n",
    "    'author_id': Feature(int64_list=Int64List(value=[123])),\n",
    "    'title': Feature(bytes_list=BytesList(value=[b\"A\", b\"desert\", b\"place\", b\".\"])),\n",
    "    'pub_date': Feature(int64_list=Int64List(value=[1623, 12, 25]))\n",
    "\n",
    "})\n",
    "content = [[\"When\", \"shall\", \"we\", \"three\", \"meet\", \"again\", \"?\"],\n",
    "           [\"In\", \"thunder\", \",\", \"lightning\", \",\", \"or\", \"in\", \"rain\", \"?\"]]\n",
    "comments = [[\"When\", \"the\", \"hurlyburly\", \"'s\", \"done\", \".\"],\n",
    "            [\"When\", \"the\", \"battle\", \"'s\", \"lost\", \"and\", \"won\", \".\"]]\n",
    "\n",
    "def words_to_feature(words):\n",
    "    return Feature(bytes_list=BytesList(value=[word.encode('utf-8') for word in words]))\n",
    "\n",
    "content_features = [words_to_feature(sentence) for sentence in content]\n",
    "comments_features = [words_to_feature(comment)  for comment in comments]\n",
    "\n",
    "sequence_example = SequenceExample(\n",
    "    context = context,\n",
    "    feature_lists=FeatureLists(feature_list={\n",
    "        'content': FeatureList(feature=content_features),\n",
    "        'comments': FeatureList(feature=comments_features)\n",
    "    })\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context {\n",
       "  feature {\n",
       "    key: \"author_id\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 123\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"pub_date\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 1623\n",
       "        value: 12\n",
       "        value: 25\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"title\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"A\"\n",
       "        value: \"desert\"\n",
       "        value: \"place\"\n",
       "        value: \".\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "feature_lists {\n",
       "  feature_list {\n",
       "    key: \"comments\"\n",
       "    value {\n",
       "      feature {\n",
       "        bytes_list {\n",
       "          value: \"When\"\n",
       "          value: \"the\"\n",
       "          value: \"hurlyburly\"\n",
       "          value: \"\\'s\"\n",
       "          value: \"done\"\n",
       "          value: \".\"\n",
       "        }\n",
       "      }\n",
       "      feature {\n",
       "        bytes_list {\n",
       "          value: \"When\"\n",
       "          value: \"the\"\n",
       "          value: \"battle\"\n",
       "          value: \"\\'s\"\n",
       "          value: \"lost\"\n",
       "          value: \"and\"\n",
       "          value: \"won\"\n",
       "          value: \".\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature_list {\n",
       "    key: \"content\"\n",
       "    value {\n",
       "      feature {\n",
       "        bytes_list {\n",
       "          value: \"When\"\n",
       "          value: \"shall\"\n",
       "          value: \"we\"\n",
       "          value: \"three\"\n",
       "          value: \"meet\"\n",
       "          value: \"again\"\n",
       "          value: \"?\"\n",
       "        }\n",
       "      }\n",
       "      feature {\n",
       "        bytes_list {\n",
       "          value: \"In\"\n",
       "          value: \"thunder\"\n",
       "          value: \",\"\n",
       "          value: \"lightning\"\n",
       "          value: \",\"\n",
       "          value: \"or\"\n",
       "          value: \"in\"\n",
       "          value: \"rain\"\n",
       "          value: \"?\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_sequence_example = sequence_example.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_feature_descriptions = {\n",
    "    \"author_id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"title\": tf.io.VarLenFeature(tf.string),\n",
    "    \"pub_date\": tf.io.FixedLenFeature([3], tf.int64, default_value=[0, 0, 0]),\n",
    "}\n",
    "\n",
    "sequence_feature_descriptions = {\n",
    "    \"content\": tf.io.VarLenFeature(tf.string),\n",
    "    \"comments\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
    "    serialized_sequence_example, context_feature_descriptions,\n",
    "    sequence_feature_descriptions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x1f88d53c2c8>,\n",
       " 'author_id': <tf.Tensor: shape=(), dtype=int64, numpy=123>,\n",
       " 'pub_date': <tf.Tensor: shape=(3,), dtype=int64, numpy=array([1623,   12,   25], dtype=int64)>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=array([b'A', b'desert', b'place', b'.'], dtype=object)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_context['title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comments': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x1f88d53cbc8>,\n",
       " 'content': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x1f88d53c288>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_feature_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'When', b'the', b'hurlyburly', b\"'s\", b'done', b'.'], [b'When', b'the', b'battle', b\"'s\", b'lost', b'and', b'won', b'.']]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.RaggedTensor.from_sparse(parsed_feature_lists['comments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Input Features\n",
    "\n",
    "If your data contains categorical features or text features, they need to be convert to numbers. This can be done ahead of time when preparing your data files, using any tool you like (e.g., Numpy, pandas, or Scikit-Learn)\n",
    "\n",
    "You can preprocess your data on the fly when loading it with the Data API (e.g. using map() method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Features Using One-Hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use the California housing dataset to demonstrate, since it contains categorical features and missing values:\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join('datasets', 'housing')\n",
    "HOUSING_URL = DOWNLOAD_ROOT + 'datasets/housing/housing.tgz'\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, 'housing.tgz')\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode categorical feature before we feed it to a neural network\n",
    "#Since there are very few categories, we ca use one-hot encoding\n",
    "\n",
    "vocab = ['<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'NEAR BAY', 'ISLAND']\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last two lines we create the lookup table, giving it the initializer and specifying the number of *out-of-vocabulary* (oov) buckets --> if we look up a category that does not exist in the vocabulary, the lookup table, will compute a hash of this category and use it to assign the unknown category to one of the oov buckets.\n",
    "\n",
    "### Why use oov buckets? \n",
    "if the number of categories is large(e.g zip codes, cities, words, products,...) and the dataset is large as well, or it keep chaning, the getting the full list of categories may not be convenient.\n",
    "\n",
    "-->> You should define the vocabulary based on a data sample (rather than the whole training set) and add some oov buckets for other categories that were not in the data sample. The more unknown categories you expect to find during training, the more oov buckets you should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = tf.constant(['NEAR BAY', 'DESERT', 'INLAND', 'INLAND'])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unknown category is \"DESERT\" was mapped to one of the two oov buckets (at index 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 7), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
    "cat_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb, if the number of categories is lower than 10, then one-hot-encoding is generally the way to go. If the number of categories is grater than 50, the embeddings are usually preferable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Features using Embeddings\n",
    "\n",
    "An embedding is trainable dense vector that represents a category. By default, embedding are initialized randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
       "array([[0.9651364 , 0.6970465 ],\n",
       "       [0.9913566 , 0.4567325 ],\n",
       "       [0.428887  , 0.40931153],\n",
       "       [0.6071029 , 0.48528087],\n",
       "       [0.80757165, 0.44847035],\n",
       "       [0.98547685, 0.2156725 ],\n",
       "       [0.1528318 , 0.336241  ]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an ebedding matrix\n",
    "embedding_dim = 2 \n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
    "embedding_matrix = tf.Variable(embed_init)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = tf.constant(['NEAR BAY', 'DESERT', 'INLAND', 'INLAND'])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[0.6071029 , 0.48528087],\n",
       "       [0.98547685, 0.2156725 ],\n",
       "       [0.9913566 , 0.4567325 ],\n",
       "       [0.9913566 , 0.4567325 ]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(embedding_matrix, cat_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lookup table says that the 'INLAND' category is at index 1, so the tf.nn.embedding_lookup() function returns the embedding at row 1 in the embedding maxtrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[-0.00730877,  0.0483962 ],\n",
       "       [ 0.03553097,  0.02830509],\n",
       "       [ 0.00728489, -0.02812772],\n",
       "       [ 0.00728489, -0.02812772]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keras provides a keras.layers.Embedding layer that handles the embedding matrix\n",
    "embedding = keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets, output_dim=embedding_dim)\n",
    "embedding(cat_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Preprocessing Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - keras.layers.Discretization layer: it will chop continuous data into different bins and encode each bin as one-hot vector.\n",
    "    For example: you could use it to discretize prices into three categories (low, medium, high) which would be encoded as [1, 0, 0], [0, 1, 0], [0, 0, 1]\n",
    "\n",
    "- The Discretization layer will not be differentiable, and it should be used at the start of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The TextVectorization layer will also have an option to output word-count vector instead of word indices. For example, the vocabulary contains three words ['and', 'basketball' , 'more'] and the text \"more and more\" will mapped to the vector [1, 0 , 2] --> ['and' * 1, 'basketball' * 0, 'more' * 2] --> this text representation is called *bag_of_words*\n",
    "\n",
    "- The word counts should be normalized in a way that reduces the importance of frequent words --> do this is to devide each word count by **the log of total number of training instances** in which the word appears --> a.k.a *Term-Frequency x Inverse-Document-Frequency* technique (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why do we use Data API?\n",
    "\n",
    "    - Bcuz it offers many features, including loading data from various sources (such as text or binary files), reading data in parallel from multiple sources, transforming it, interleaving the records, shuffling the data, batching it, and prefetching it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the benefits of splitting a large dataset into multiple files?\n",
    "\n",
    "    - Makes it possible to shuffle it at a coarse level before shuffling it at a finer level using a shuffling buffer.\n",
    "\n",
    "    - Handle huge datasets that do not fit on a single machine.\n",
    "\n",
    "    - Easy to manipulate thousands of small files rather than one huge files\n",
    "\n",
    "    - If the data is split across multiple files spread across multiple servers, it is possoble to download several files from different servers simultaneously, which improves the bandwidth usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?\n",
    "\n",
    "    - I can use TensorBoard to visualize profiling data: if the GPU is not fully utilized then your input pipeline is likely to be the bottleneck --> fix it by making sure it read and preprocesses the data in multiple threads in parallel, and esuring it prefetches a few batches. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
