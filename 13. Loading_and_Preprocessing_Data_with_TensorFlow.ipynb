{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python37\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#import some libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data API\n",
    "\n",
    "The whole Data API revolves around the concept of a *dataset* which represents a sequence of data items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset\n",
    "\n",
    "\"\"\"\n",
    "Usually you will use datasets that gradually read from disk, but the dataset you saw above is created entirely in RAM.\n",
    "The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset whose elements are all the slices of X\n",
    "\"\"\"\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7, drop_remainder=True)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "    1. repeat(n) method: it returns a new dataset that will repeat the items of the original dataset n times. You can call this method with no arguments, the new dataset will repeat the source dataset forever, so the code that iterates over the dataset will have to decide when to stop\n",
    "\n",
    "    2. batch(n) method: it will group the items of the previous dataset in batches of n times, drop_reminder=True will be called if you want to drop the batch don't have the exact same size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Creating new dataset with map() method\n",
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-a1725821a0f0>:6: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.unbatch()`.\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Creating new dataset with apply() method\n",
    "\"\"\" \n",
    "The map() method applies a transformation to each item, the apply() method applies a transformation to the dataset as a whole\n",
    "\"\"\"\n",
    "\n",
    "dataset = dataset.apply(tf.data.experimental.unbatch())\n",
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle() method: it will create a new dataset that will start by filling up a buffer with the first items of the source dataset. Then, whenever it is asked for an item, it will pull one out randomly from the buffer and replace it with a fresh one from the source datatset. \n",
    "\n",
    ".The buffer_size must be specified, and it is important to make it large enough, or else shuffling will not be very effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7], shape=(5,), dtype=int64)\n",
      "tf.Tensor([9 1 8 4 5], shape=(5,), dtype=int64)\n",
      "tf.Tensor([3 5 2 1 8], shape=(5,), dtype=int64)\n",
      "tf.Tensor([4 0 7 9 6], shape=(5,), dtype=int64)\n",
      "tf.Tensor([2 1 3 5 8], shape=(5,), dtype=int64)\n",
      "tf.Tensor([9 4 6 0 7], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).repeat(3).batch(5)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interleaving lines from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load California housing dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
      "   0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
      "   0.8252202 ]\n",
      " [-0.40342651  2.99178419 -1.33391162 -0.25683275 -1.21518188  1.89434613\n",
      "  -1.91036058  1.24758524 -0.85646254 -0.34843254 -1.71818909  0.43190599\n",
      "  -1.32920239]\n",
      " [ 0.1249402  -0.48361547  1.0283258  -0.25683275  0.62864202 -1.82968811\n",
      "   1.11048828 -1.18743907  1.67588577  1.5652875   0.78447637  0.22061726\n",
      "  -1.30850006]\n",
      " [-0.40149354 -0.48361547 -0.86940196 -0.25683275 -0.3615597  -0.3245576\n",
      "  -1.23667187  1.10717989 -0.51114231 -1.094663    0.78447637  0.44807713\n",
      "  -0.65292624]\n",
      " [-0.0056343  -0.48361547  1.0283258  -0.25683275  1.32861221  0.15364225\n",
      "   0.69480801 -0.57857203  1.67588577  1.5652875   0.78447637  0.3898823\n",
      "   0.26349695]\n",
      " [-0.37502238 -0.48361547 -0.54747912 -0.25683275 -0.54935658 -0.78865126\n",
      "   0.18954148  0.48371503 -0.51114231 -0.71552978  0.51145832  0.38669063\n",
      "  -0.13812828]\n",
      " [ 0.58963463 -0.48361547  1.0283258  -0.25683275  1.21764133 -1.03127774\n",
      "   1.11048828 -1.06518235  1.67588577  1.5652875   0.78447637  0.44807713\n",
      "   1.49873604]\n",
      " [ 0.0381708  -0.48361547  1.24588095 -0.25683275  2.67733525 -1.12719983\n",
      "   1.11048828 -1.14833073 -0.51114231 -0.01744323 -1.71818909  0.44807713\n",
      "   1.88793986]\n",
      " [-0.17228416 -0.48361547  1.24588095 -0.25683275  2.67733525 -0.90150078\n",
      "   1.11048828 -1.09664657 -0.51114231 -0.01744323 -1.71818909 -1.97365769\n",
      "   0.53952803]\n",
      " [-0.22932104 -0.48361547  1.58544339 -0.25683275  0.56888847 -1.76056777\n",
      "   1.11048828 -1.13471925 -0.62624905  0.18716835  1.23950646  0.44807713\n",
      "   2.99068404]]\n"
     ]
    }
   ],
   "source": [
    "# Scale the dataset to normalize the input values\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_full)\n",
    "X_train_new = scaler.transform(X_train_full)\n",
    "X_test_new = scaler.transform(X_test)\n",
    "print(X_train_new[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_new, y_train_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done writing files. Traning file paths: ['datasets\\\\boston\\\\my_train_00.csv', 'datasets\\\\boston\\\\my_train_01.csv', 'datasets\\\\boston\\\\my_train_02.csv', 'datasets\\\\boston\\\\my_train_03.csv', 'datasets\\\\boston\\\\my_train_04.csv', 'datasets\\\\boston\\\\my_train_05.csv', 'datasets\\\\boston\\\\my_train_06.csv', 'datasets\\\\boston\\\\my_train_07.csv', 'datasets\\\\boston\\\\my_train_08.csv', 'datasets\\\\boston\\\\my_train_09.csv', 'datasets\\\\boston\\\\my_train_10.csv', 'datasets\\\\boston\\\\my_train_11.csv', 'datasets\\\\boston\\\\my_train_12.csv', 'datasets\\\\boston\\\\my_train_13.csv', 'datasets\\\\boston\\\\my_train_14.csv', 'datasets\\\\boston\\\\my_train_15.csv', 'datasets\\\\boston\\\\my_train_16.csv', 'datasets\\\\boston\\\\my_train_17.csv', 'datasets\\\\boston\\\\my_train_18.csv', 'datasets\\\\boston\\\\my_train_19.csv']\n"
     ]
    }
   ],
   "source": [
    "#Save dataset to multiple files\n",
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join('datasets', \"boston\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding='utf-8') as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths\n",
    "\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, 'train', None, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, 'valid', None, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, 'test', None, n_parts=10)\n",
    "print('\\nDone writing files. Traning file paths:', train_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataset containing only these filepaths above:\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
    "\n",
    "\"\"\"\n",
    "By default, the list_files() function returns a dataset that shuffles the filepaths\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-0.3919630095351413,0.7801662253156655,-0.9076211063403838,-0.2568327484687563,-1.1042109960227324,0.17762277400333037,-2.193453178811165,1.625106473803,-0.3960355701527182,-0.6372959438833558,-0.8536319333458502,0.21199974653499268,-1.0421300691886186,23.7'\n",
      "b'0.34856843562344747,-0.4836154708652843,1.0283257954396188,-0.2568327484687563,1.2176413250911147,-0.7815981666935935,1.0029847632968047,-0.8944963977369444,1.6758857724016463,1.5652874992218142,0.7844763709927688,0.42179902963935195,0.6030151782242079,15.1'\n",
      "b'0.11561228941989592,-0.4836154708652843,1.0283257954396188,-0.2568327484687563,1.3286122080855265,0.6120934353778194,0.7521432207546996,-0.5635796768907313,1.6758857724016463,1.5652874992218142,0.7844763709927688,-1.0591370863914173,0.5077844550098752,16.4'\n",
      "b'0.4670650820425747,-0.4836154708652843,1.0283257954396188,-0.2568327484687563,0.2274395999102092,-1.185035209398477,0.9456495535728953,-0.646284198628302,1.6758857724016463,1.5652874992218142,0.7844763709927688,-0.023439306969036875,0.745171185341256,13.8'\n",
      "b'-0.4012313273862551,0.6959141122369354,0.5785158091490484,-0.2568327484687563,-0.7969070123458994,0.24674310649472692,-0.5522328004642475,-0.036677404830978554,-0.6262490526587586,-0.8178355703673861,-0.12558379808424228,0.42796959678948,-0.6322239127443154,22.9'\n"
     ]
    }
   ],
   "source": [
    "n_files_in_1_read = 5\n",
    "dataset = filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_files_in_1_read)\n",
    "\n",
    "\"\"\"\n",
    "The interleave() function takes a function that returns a dataset, and applies it to each filepath in the dataset.\n",
    "The function is applied to each filepath in the dataset, and the resulting datasets are interleaved in the order of the filepaths.\n",
    "By default, interleave() does not use parallelism; it just reads one line at a time from each file, squentially.\n",
    "\"\"\"\n",
    "\n",
    "for line in dataset.take(5):\n",
    "    print(line.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Data\n",
    "n_inputs = X_train.shape[-1]\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return x , y\n",
    "\n",
    "\"\"\" \n",
    "tf.io.decode_csv() function which takes two arguments:\n",
    "    1. The line to parse\n",
    "    2. An array containing the default value for each column in the CSV file. This array tells TensorFlow not inly the default value for each column, but also the number of columns and their types.\n",
    "\n",
    "The decode__csv() function returns a list of scalar tensors, one for each column in the CSV file., but we need to return 1D tensor arrays.\n",
    "    --> Call tf.stack() on all tensor except the last one (y): this will stack these tensors into a 1D array\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5, n_read_threads=None, shuffle_buffer_size=10000, n_parse_threads=5, batch_size=5):\n",
    "    #create a dataset of filepaths\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    #Create a datasets with shuffled filepaths\n",
    "    dataset  = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    #Preprocess the data\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    #shuffle the data\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    #batch the data\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefetching\n",
    "\n",
    "While our training algorithm is working on one batch, the dataset will already be working in parllel on getting the next bacth ready --> can improve performance dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset is small enough to fit in memory --> using the dataset's cache() method to cache its content to RAM, do this:\n",
    "\n",
    "**After** loading and preprocessing the data\n",
    "**Before** shuffling, repeating, batching and prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
