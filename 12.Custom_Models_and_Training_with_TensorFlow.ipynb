{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow like Numpy\n",
    "### A tensor is usally a multidimensonal array, but it can also hold a scalar (vô hướng) (a simple value, such as 42). These tensors will be important when we create custom cost functions, custom metrics, custom layers, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[1., 2., 3.], [4., 5., 6.]]) #Matrix  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: These value above show us the shape of matrix with 2 rows and 3 columns (shape =(rows, columns)), and data types=float32 as well as the numpy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2. 3.]\n",
      " [5. 6.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Indexing works much like in Numpy\n",
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "print(t[0, :]) #[1. 2. 3.]\n",
    "print(t[:, 1:]) #[[2. 3.], [5. 6.]] (columns 1 and 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: We can see the difference betweem two shape of array. With the shape of (3,), it is a 1-ranked array not a dimensional array (e.g shape=(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[11. 12. 13.]\n",
      " [14. 15. 16.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 1.  4.  9.]\n",
      " [16. 25. 36.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(21.0, shape=(), dtype=float32)\n",
      "tf.Tensor([5. 7. 9.], shape=(3,), dtype=float32)\n",
      "tf.Tensor([ 6. 15.], shape=(2,), dtype=float32)\n",
      "tf.Tensor(3.5, shape=(), dtype=float32)\n",
      "tf.Tensor([2.5 3.5 4.5], shape=(3,), dtype=float32)\n",
      "tf.Tensor([2. 5.], shape=(2,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 4.]\n",
      " [2. 5.]\n",
      " [3. 6.]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# All sorts of tensor operations are available:\n",
    "print(t + 10) #add 10 to each element\n",
    "print(tf.square(t)) #square each element\n",
    "print(tf.reduce_sum(t)) #sum all elements\n",
    "print(tf.reduce_sum(t, 0)) #sum each column\n",
    "print(tf.reduce_sum(t, 1)) #sum each row\n",
    "print(tf.reduce_mean(t)) #mean of all elements\n",
    "print(tf.reduce_mean(t, 0)) #mean of each column\n",
    "print(tf.reduce_mean(t, 1)) #mean of each row\n",
    "print(tf.transpose(t)) #swap rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and Numpy\n",
    "### You can create a tensor from a NumPy array, and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2. 4. 5.], shape=(3,), dtype=float64)\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([2., 4., 5.])\n",
    "print(tf.constant(a)) #[2. 4. 5.]\n",
    "print(t.numpy()) #[1. 2. 3.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 4. 16. 25.], shape=(3,), dtype=float64)\n",
      "[[ 1.  4.  9.]\n",
      " [16. 25. 36.]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.square(a)) #[4. 16. 25.]\n",
    "print(np.square(t)) #[[1. 4. 9.], [16. 25. 36.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: Numpy uses 64-bit precision by default, while TensorFlow uses 32-bit precision (more than enough for neural networks, plus it faster and uses less RAM). So when we create a tensor from a NumPy array, make sure to set dtype=tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using tf.cast() to convert between types\n",
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32) #42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "### The tf.Tensor values we have seen so far ar immutable (you can not modify them). This means that we cannot use regular tensors to implement weights in a neural network, since they need to be tweaked by backpropagation. That's why we need tf.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "A Tf.Variable acts much like a tf.Tensor: you can perform the same operation with it, it plays nicely with NumPy as well.\n",
    "But it can also be modified in place using tf.assign().\n",
    "\"\"\"\n",
    "v.assign(2 *  v)    # v is now [[2., 4., 6.], [8., 10., 12.]]\n",
    "v[0,1].assign(42.)  # v is now [[2., 42., 6.], [8., 10., 12.]]\n",
    "v[:, 2].assign([0., 1.])    # v is now [[2., 42., 0.], [8., 10., 1.]]\n",
    "v.scatter_nd_update([[0, 0], [1, 2]], updates=[100., 200.]) # v is now [[100., 42., 0.], [8., 10., 200.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([100. 200.], shape=(2,), dtype=float32), dense_shape=tf.Tensor([3 3], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#Sparse tensors\n",
    "#A sparse tensor is a tensor that has a very small number of elements, and most of them are zero.\n",
    "t = tf.SparseTensor([[0, 0], [1, 2]], [100., 200.], [3, 3])\n",
    "print(t) #SparseTensor(indices=[[0 0], [1 2]], values=[100. 200.], dense_shape=[3 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method TensorArray.read of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000022D86020488>>\n"
     ]
    }
   ],
   "source": [
    "#Tensor Arrays\n",
    "#A tensor array is a tensor that can be dynamically resized.\n",
    "ta = tf.TensorArray(tf.float32, size=3)\n",
    "ta = ta.write(0, [[1., 2.], [3., 4.]])\n",
    "print(ta.read) #TensorArray(size=3, dtype=float32, dynamic_size=False, clear_after_read=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[1, 2], [3], [4, 5], [6]]>\n"
     ]
    }
   ],
   "source": [
    "#Ragged Tensor\n",
    "#A ragged tensor is a tensor that has a variable number of dimensions.\n",
    "rt = tf.RaggedTensor.from_row_splits(values=[1, 2, 3, 4, 5, 6], row_splits=[0, 2, 3, 5, 6])\n",
    "print(rt) #RaggedTensor(values=Tensor(values, dtype=int32), row_splits=Tensor(row_splits, dtype=int64))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'Hello' b'World'], shape=(2,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#String Tensors\n",
    "\"\"\"\n",
    "A string tensor are regular tensors of type tf.string.\n",
    "These represent byte strings, not Unicode strings. Alternatively, you can respresent Unicode strings a using tf.int32, where each item represents a Unicode code point (a 32-bit integer). \n",
    "\"\"\"\n",
    "st = tf.constant([\"Hello\", \"World\"])\n",
    "print(st) #Tensor(\"Const:0\", shape=(2,), dtype=string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape(None)]\n"
     ]
    }
   ],
   "source": [
    "# Queues\n",
    "\"\"\"\n",
    "Store tensors across multiple steps in a computation.\n",
    "First In, First Out (FIFO) queues.\n",
    "\"\"\"\n",
    "q = tf.queue.FIFOQueue(3, \"float\")\n",
    "print(q.shapes) #FIFOQueue(3, dtype=float32, shapes=None, shared_name=None, name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing Models and Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Function\n",
    "Suppose you start by trying to clean up your dataset by removing or fixing the outliers, but that turns out to be unsufficient; the dataset is still noisy.Which loss function should you use?\n",
    "\n",
    "    1. Mean Squared Error (MSE) --> might penalize large errors to much and cause the model to be imprecise\n",
    "    \n",
    "    2. Mean Absolute Error (MAE) --> would not penalize outliers as much, but training might take a while to converge, and the trained model might not be very precise\n",
    "\n",
    "    3. Huber Loss --> a combination of MSE and MAE, is quadratic when the error is smaller than a threshold but linear when the error is larger than threshold. The linear part makes it less sensitive to outliers than the MSE, and the quadratic part allows it to converge faster and be more precise than the MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Huber loss \n",
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "t = create_huber()\n",
    "t(1., 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: For better performance, you should use a vectorized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When you save the model, the threshold will not be saved.\n",
    "You have to set it again when loading the model.\n",
    "To solve this problem, you create a subclass of the keras.losses.Loss class and then implementing its get_config() method.\n",
    "\"\"\"\n",
    "\n",
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super(HuberLoss, self).__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        config = {'threshold': self.threshold}\n",
    "        base_config = super(HuberLoss, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
