{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow like Numpy\n",
    "### A tensor is usally a multidimensonal array, but it can also hold a scalar (vô hướng) (a simple value, such as 42). These tensors will be important when we create custom cost functions, custom metrics, custom layers, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[1., 2., 3.], [4., 5., 6.]]) #Matrix  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: These value above show us the shape of matrix with 2 rows and 3 columns (shape =(rows, columns)), and data types=float32 as well as the numpy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2. 3.]\n",
      " [5. 6.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Indexing works much like in Numpy\n",
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "print(t[0, :]) #[1. 2. 3.]\n",
    "print(t[:, 1:]) #[[2. 3.], [5. 6.]] (columns 1 and 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: We can see the difference betweem two shape of array. With the shape of (3,), it is a 1-ranked array not a dimensional array (e.g shape=(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[11. 12. 13.]\n",
      " [14. 15. 16.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 1.  4.  9.]\n",
      " [16. 25. 36.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(21.0, shape=(), dtype=float32)\n",
      "tf.Tensor([5. 7. 9.], shape=(3,), dtype=float32)\n",
      "tf.Tensor([ 6. 15.], shape=(2,), dtype=float32)\n",
      "tf.Tensor(3.5, shape=(), dtype=float32)\n",
      "tf.Tensor([2.5 3.5 4.5], shape=(3,), dtype=float32)\n",
      "tf.Tensor([2. 5.], shape=(2,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 4.]\n",
      " [2. 5.]\n",
      " [3. 6.]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# All sorts of tensor operations are available:\n",
    "print(t + 10) #add 10 to each element\n",
    "print(tf.square(t)) #square each element\n",
    "print(tf.reduce_sum(t)) #sum all elements\n",
    "print(tf.reduce_sum(t, 0)) #sum each column\n",
    "print(tf.reduce_sum(t, 1)) #sum each row\n",
    "print(tf.reduce_mean(t)) #mean of all elements\n",
    "print(tf.reduce_mean(t, 0)) #mean of each column\n",
    "print(tf.reduce_mean(t, 1)) #mean of each row\n",
    "print(tf.transpose(t)) #swap rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and Numpy\n",
    "### You can create a tensor from a NumPy array, and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2. 4. 5.], shape=(3,), dtype=float64)\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([2., 4., 5.])\n",
    "print(tf.constant(a)) #[2. 4. 5.]\n",
    "print(t.numpy()) #[1. 2. 3.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 4. 16. 25.], shape=(3,), dtype=float64)\n",
      "[[ 1.  4.  9.]\n",
      " [16. 25. 36.]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.square(a)) #[4. 16. 25.]\n",
    "print(np.square(t)) #[[1. 4. 9.], [16. 25. 36.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: Numpy uses 64-bit precision by default, while TensorFlow uses 32-bit precision (more than enough for neural networks, plus it faster and uses less RAM). So when we create a tensor from a NumPy array, make sure to set dtype=tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using tf.cast() to convert between types\n",
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32) #42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "### The tf.Tensor values we have seen so far ar immutable (you can not modify them). This means that we cannot use regular tensors to implement weights in a neural network, since they need to be tweaked by backpropagation. That's why we need tf.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "A Tf.Variable acts much like a tf.Tensor: you can perform the same operation with it, it plays nicely with NumPy as well.\n",
    "But it can also be modified in place using tf.assign().\n",
    "\"\"\"\n",
    "v.assign(2 *  v)    # v is now [[2., 4., 6.], [8., 10., 12.]]\n",
    "v[0,1].assign(42.)  # v is now [[2., 42., 6.], [8., 10., 12.]]\n",
    "v[:, 2].assign([0., 1.])    # v is now [[2., 42., 0.], [8., 10., 1.]]\n",
    "v.scatter_nd_update([[0, 0], [1, 2]], updates=[100., 200.]) # v is now [[100., 42., 0.], [8., 10., 200.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([100. 200.], shape=(2,), dtype=float32), dense_shape=tf.Tensor([3 3], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#Sparse tensors\n",
    "#A sparse tensor is a tensor that has a very small number of elements, and most of them are zero.\n",
    "t = tf.SparseTensor([[0, 0], [1, 2]], [100., 200.], [3, 3])\n",
    "print(t) #SparseTensor(indices=[[0 0], [1 2]], values=[100. 200.], dense_shape=[3 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method TensorArray.read of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000022D86020488>>\n"
     ]
    }
   ],
   "source": [
    "#Tensor Arrays\n",
    "#A tensor array is a tensor that can be dynamically resized.\n",
    "ta = tf.TensorArray(tf.float32, size=3)\n",
    "ta = ta.write(0, [[1., 2.], [3., 4.]])\n",
    "print(ta.read) #TensorArray(size=3, dtype=float32, dynamic_size=False, clear_after_read=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[1, 2], [3], [4, 5], [6]]>\n"
     ]
    }
   ],
   "source": [
    "#Ragged Tensor\n",
    "#A ragged tensor is a tensor that has a variable number of dimensions.\n",
    "rt = tf.RaggedTensor.from_row_splits(values=[1, 2, 3, 4, 5, 6], row_splits=[0, 2, 3, 5, 6])\n",
    "print(rt) #RaggedTensor(values=Tensor(values, dtype=int32), row_splits=Tensor(row_splits, dtype=int64))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'Hello' b'World'], shape=(2,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#String Tensors\n",
    "\"\"\"\n",
    "A string tensor are regular tensors of type tf.string.\n",
    "These represent byte strings, not Unicode strings. Alternatively, you can respresent Unicode strings a using tf.int32, where each item represents a Unicode code point (a 32-bit integer). \n",
    "\"\"\"\n",
    "st = tf.constant([\"Hello\", \"World\"])\n",
    "print(st) #Tensor(\"Const:0\", shape=(2,), dtype=string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape(None)]\n"
     ]
    }
   ],
   "source": [
    "# Queues\n",
    "\"\"\"\n",
    "Store tensors across multiple steps in a computation.\n",
    "First In, First Out (FIFO) queues.\n",
    "\"\"\"\n",
    "q = tf.queue.FIFOQueue(3, \"float\")\n",
    "print(q.shapes) #FIFOQueue(3, dtype=float32, shapes=None, shared_name=None, name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing Models and Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Function\n",
    "Suppose you start by trying to clean up your dataset by removing or fixing the outliers, but that turns out to be unsufficient; the dataset is still noisy.Which loss function should you use?\n",
    "\n",
    "    1. Mean Squared Error (MSE) --> might penalize large errors to much and cause the model to be imprecise\n",
    "    \n",
    "    2. Mean Absolute Error (MAE) --> would not penalize outliers as much, but training might take a while to converge, and the trained model might not be very precise\n",
    "\n",
    "    3. Huber Loss --> a combination of MSE and MAE, is quadratic when the error is smaller than a threshold but linear when the error is larger than threshold. The linear part makes it less sensitive to outliers than the MSE, and the quadratic part allows it to converge faster and be more precise than the MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Huber loss \n",
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "t = create_huber()\n",
    "t(1., 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: For better performance, you should use a vectorized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When you save the model, the threshold will not be saved.\n",
    "You have to set it again when loading the model.\n",
    "To solve this problem, you create a subclass of the keras.losses.Loss class and then implementing its get_config() method.\n",
    "\"\"\"\n",
    "\n",
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super(HuberLoss, self).__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        config = {'threshold': self.threshold}\n",
    "        base_config = super(HuberLoss, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "when you save the model, the threshold will be saved along with it; and when you load the model, you just need to map the class name to the class itself.\n",
    "\"\"\"\n",
    "\n",
    "model = keras.models.load_model('my_model_with_a_custom_class.h5', custom_objects={'HuberLoss': HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: Brief notation about *args and **kwargs\n",
    "\n",
    "    - args are used to pass *non-keyword* arguments, which are usually used as a measure to prevent the program from crashing if we don't know how many arguments will be passed to the function\n",
    "\n",
    "    - **kwargs is a dicitonary of keyword arguments. The ** allows us to pass any number of keyword arguments. A keyword argument is basically a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Actovation Functions, Initializers, Regularizers and Constaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Soft plus activation function\n",
    "def my_softplus(x): # return value is just tf.nn.softplus(x)\n",
    "    return tf.math.log(1. + tf.exp(x))\n",
    "\n",
    "#A custom Glorot Initializer\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2.0 / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "#A custom L1 regularizer\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(weights * 0.01))\n",
    "\n",
    "#A custom weight\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0, tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Some differences between losses and metrics:\n",
    "\n",
    "    1. Losses are used by Gradient Descent to train a model, so they must be differentiable, and their gradients should not be 0 everywhere.\n",
    "    To recall, when we train the model, the model will perform backpropagation to update the weights and minimize the loss. So if the gradient is 0 everywhere, the model will not update the weights.\n",
    "\n",
    "    2. Metrics are used to evaluate the model, so they are not differentiable or have 0 gradients everywhere.\n",
    "\"\"\"\n",
    "#An object that can keep track of the number of true positives and false positives and that can compute their ratio when requested.\n",
    "precision = keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1]) # first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0]) # second batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: After the first batch, it returns a precision of 80%; then after the second batch, it return 50% (which is the overall precision so far, not the second batch's precision) --> a.k.a *streaming metric* (or *stateful metric*), as it is gradually updated, batch after batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result() # 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A simplified version of the custom Dense layer\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=[batch_input_shape[-1], self.units],\n",
    "                                      initializer=my_glorot_initializer)\n",
    "        self.bias = self.add_weight(name='bias',\n",
    "                                    shape=[self.units],\n",
    "                                    initializer='zeros')\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "    def call(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.kernel) + self.bias)\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape([batch_input_shape[0], self.units])\n",
    "    def get_config(self):\n",
    "        base_config = super(MyDense, self).get_config()\n",
    "        return dict(list(base_config.items()) + list({'units': self.units}.items()) + {\"activation\": keras.activations.serialize(self.activation)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: A custom layer class will have 5 main functions:\n",
    "\n",
    "    1. The constructor (__init__) --> takes all the hyperparameters as arguments, and importantly it also takes a **kwargs argument\n",
    "\n",
    "    2. The build() method --> Its role is to create the layer's variables by calling the add_weight() method for each weight.\n",
    "\n",
    "    3. The call() method --> performs the desired operations. For example, we will compute the output of the layer by performing the linear equation (Wx + b) and applying the activation to it.\n",
    "\n",
    "    4.The compute_output_shape() method --> simply returns the shape of this layer's outputs\n",
    "\n",
    "    5. The get_config() method -- returns a dictionary mapping each hyperparameter name to its value. It first calls the parent class's get_config() method, then adds the new hyperparameters to this dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to build a custome model represented in the below picture like this:\n",
    "\n",
    "![Custom_models.png](Custom_models.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a custom ResidualBlock layer\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation='elu',kernel_initializer=my_glorot_initializer, bias_initializer='zeros') for _ in range(n_layers)]\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z\n",
    "    \n",
    "    def  get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super(ResidualRegressor, self).__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation='elu', kernel_initializer='he_normal')\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return super().get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: The Model class is a subclass of the Layer class, so models can be defined and used exactly like layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses and Metrics Based on Model Internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A custom regression MLP model composed of a stack of five hidden layers plus an output layer\n",
    "\"\"\"\n",
    "This custom model will also have an auxiliary output on top of the upper hidden layer. The loss associated to this auxiliary output will be called reconstruction loss\n",
    "Reconstruction loss is the mean squared difference between the reconstruction and the inputs.\n",
    "\"\"\"\n",
    "\n",
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation='selu', kernel_initializer='lecun_normal') for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "        super().build(batch_input_shape)\n",
    "    \n",
    "    def call (self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients Using Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural network, the function would be much more complex, typically with tens thousands of parameters, and finding the partial derivatives anylytically by hand would be an almost impossible task. One solution could be to compute an approximation of each partial derivative by measuring how much the function's output changes when you tweak the corresponding parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.000003007075065\n"
     ]
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2  * w1 * w2\n",
    "\n",
    "w1, w2 =  5, 3\n",
    "eps = 1e-6\n",
    "print((f(w1 + eps, w2) - f(w1, w2)) / eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needing to call f() at least once per parameter makes this approach intractable for large neural network --> we should use autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>, <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n"
     ]
    }
   ],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tape automatically erased immediately after gradient() method was called, so exception will throw if you call twice.\n",
    "\n",
    "--> Solution: make the tape persistent with tf.GradientTape(persistent=True) and delete it each time you are done with it to free resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2)\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can force the tape to watch any tensors you like, to record every operation that involves them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.) \n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: \n",
    "\n",
    "    1. The tape's jabobian() method: it will perform reverse-mode autodiff once for each loss in the vector.\n",
    "\n",
    "    2. Reverse-mode autodiff: it just needs to do one forward pass and one reverse pass to get all the gradients at once\n",
    "\n",
    "    3. The Hessians method: the partial derivatives of the partial derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading and preparing the Fashion MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full.astype(np.float32) / 255.\n",
    "X_train, X_valid = X_train_full[5000:], X_train_full[:5000]\n",
    "y_train, y_valid = y_train_full[5000:], y_train_full[:5000]\n",
    "X_test = X_test.astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = keras.regularizers.l2(0.01)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random batch_size function\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disply the training status function\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = ' - '.join([\"{}: {:.4f}\".format(m.name, m.result()) for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration + total) + metrics, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-4d78c9d6084b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mprint_status_bar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mprint_status_bar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmean_loss\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-0c6cfa747d8f>\u001b[0m in \u001b[0;36mprint_status_bar\u001b[1;34m(iteration, total, loss, metrics)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' - '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"{}: {:.4f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r{}/{} - \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "for epoch in range(1 , n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Functions and Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating simple function to visualize how to use tf.function()\n",
    "#Simple function without tf.function()\n",
    "def cube(x):\n",
    "    return x ** 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try with tf.constant()\n",
    "cube(tf.constant(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.eager.def_function.Function object at 0x0000021985152208>\n"
     ]
    }
   ],
   "source": [
    "#Let's use tf.function() to convert this Python function to TensorFlow function\n",
    "tf_cube = tf.function(cube)\n",
    "print(tf_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: TF function can then be used exactly like the original Python function, and it will return the same result (but as tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using tf.function as a decorator\n",
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: As a result, a TF function will usually run much faster than the original Python function, especially if it perform complex computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=1000>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8000>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1000, 8000])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant([10, 20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: if a TF function was called many times with different numerical Python values, then any graph will be generated, slowing down the program and using up a lot of RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGraph and Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*AutoGraph*: TensorFlow generates graphs by analyzing th Python function's source code to capture all the control flow statements (for, while loops, and if statements, break, continue, return statments).\n",
    "\n",
    "**Question**: What is the reason TensorFlow has analyze this source code?\n",
    "\n",
    "**Answer**: The reason TensorFlow has analyze the source code is that Python does not provide any other way to capture control flow statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sum_squares(n):\n",
    "    s = 0\n",
    "    for i in tf.range(n + 1):\n",
    "        s += i ** 2\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=55>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_squares(tf.constant(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'n' type=Placeholder>,\n",
       " <tf.Operation 'add/y' type=Const>,\n",
       " <tf.Operation 'add' type=AddV2>,\n",
       " <tf.Operation 'range/start' type=Const>,\n",
       " <tf.Operation 'range/delta' type=Const>,\n",
       " <tf.Operation 'range' type=Range>,\n",
       " <tf.Operation 'sub' type=Sub>,\n",
       " <tf.Operation 'floordiv' type=FloorDiv>,\n",
       " <tf.Operation 'mod' type=FloorMod>,\n",
       " <tf.Operation 'zeros_like' type=Const>,\n",
       " <tf.Operation 'NotEqual' type=NotEqual>,\n",
       " <tf.Operation 'Cast' type=Cast>,\n",
       " <tf.Operation 'add_1' type=AddV2>,\n",
       " <tf.Operation 'zeros_like_1' type=Const>,\n",
       " <tf.Operation 'Maximum' type=Maximum>,\n",
       " <tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'while/maximum_iterations' type=Const>,\n",
       " <tf.Operation 'while/loop_counter' type=Const>,\n",
       " <tf.Operation 'while' type=StatelessWhile>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_squares.get_concrete_function(tf.constant(5)).graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def tf__sum_squares(n):\n",
      "    with ag__.FunctionScope('sum_squares', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        s = 0\n",
      "\n",
      "        def get_state():\n",
      "            return (s,)\n",
      "\n",
      "        def set_state(vars_):\n",
      "            nonlocal s\n",
      "            (s,) = vars_\n",
      "\n",
      "        def loop_body(itr):\n",
      "            nonlocal s\n",
      "            i = itr\n",
      "            s = ag__.ld(s)\n",
      "            s += (i ** 2)\n",
      "        i = ag__.Undefined('i')\n",
      "        ag__.for_stmt(ag__.converted_call(ag__.ld(tf).range, ((ag__.ld(n) + 1),), None, fscope), None, loop_body, get_state, set_state, ('s',), {'iterate_names': 'i'})\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(s)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tf.autograph.to_code(sum_squares.python_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Function Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few rules to respect:\n",
    "\n",
    "    . A TensorFlow graph **can** only include TensorFlow constructs (tensors, operations, variables, datasets,...) --> Make sure use tf.reduce_sum() instead of np.sum()\n",
    "\n",
    "    . Other Python functions or TF functions can be called, but they should follow the same rules, as TensorFlow will capture their operations in the computation graph\n",
    "\n",
    "    . If the function creates a TensorFlow variable (or any other stateful TensorFLow object, such as a dataset or a queue), it must do so upon the very first call, and only then, or else an exception will be thrown.\n",
    "\n",
    "    . TensorFlow will only capture for loops that iterate over a tensor or a dataset.So make so using *for i in tf.range(x)* rather than *for i in range(x)*, or else the loop will not be capture in the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
